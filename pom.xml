<?xml version="1.0" encoding="UTF-8"?>

<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.shockang.study.spark</groupId>
    <artifactId>spark-examples</artifactId>
    <version>1.0-SNAPSHOT</version>

    <name>spark-examples</name>
    <url>https://www.shockang.com</url>

    <properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
        <java.version>8</java.version>
        <maven.compiler.source>${java.version}</maven.compiler.source>
        <maven.compiler.target>${java.version}</maven.compiler.target>
        <maven.version>3.6.3</maven.version>
        <scala.version>2.12.15</scala.version>
        <scala.binary.version>2.12</scala.binary.version>

        <scalatest.version>3.2.9</scalatest.version>
        <scalatest-maven-plugin.version>2.0.2</scalatest-maven-plugin.version>
        <test.java.home>${java.home}</test.java.home>
        <test_classpath>target</test_classpath>
        <spark.test.home></spark.test.home>

        <CodeCacheSize>4g</CodeCacheSize>
        <extraJavaTestArgs></extraJavaTestArgs>

        <!-- Some UI tests require Chrome and Chrome driver installed so those tests are disabled by default. -->
        <test.default.exclude.tags>org.apache.spark.tags.ChromeUITest</test.default.exclude.tags>
        <test.exclude.tags></test.exclude.tags>
        <test.include.tags></test.include.tags>

        <spark.version>3.3.0-SNAPSHOT</spark.version>

        <slf4j.version>1.7.30</slf4j.version>
        <log4j.version>1.2.17</log4j.version>

        <gson.version>2.9.0</gson.version>
        <lombok.version>1.18.22</lombok.version>
        <guava.version>31.1-jre</guava.version>
    </properties>

    <dependencies>
        <!-- scala start -->
        <dependency>
            <groupId>org.scala-lang.modules</groupId>
            <artifactId>scala-xml_${scala.binary.version}</artifactId>
            <version>1.2.0</version>
        </dependency>
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-compiler</artifactId>
            <version>${scala.version}</version>
        </dependency>
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-reflect</artifactId>
            <version>${scala.version}</version>
        </dependency>
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
            <version>${scala.version}</version>
        </dependency>
        <dependency>
            <groupId>org.scala-lang.modules</groupId>
            <artifactId>scala-parser-combinators_${scala.binary.version}</artifactId>
            <version>1.1.2</version>
        </dependency>
        <!-- scala end -->

        <!-- scala test start -->
        <dependency>
            <groupId>org.scalatest</groupId>
            <artifactId>scalatest_${scala.binary.version}</artifactId>
            <version>${scalatest.version}</version>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.scalatestplus</groupId>
            <artifactId>scalacheck-1-15_${scala.binary.version}</artifactId>
            <version>3.2.9.0</version>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.scalatestplus</groupId>
            <artifactId>mockito-3-4_${scala.binary.version}</artifactId>
            <version>3.2.9.0</version>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.scalatestplus</groupId>
            <artifactId>selenium-3-141_${scala.binary.version}</artifactId>
            <version>3.2.9.0</version>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <version>4.13.1</version>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>com.novocode</groupId>
            <artifactId>junit-interface</artifactId>
            <version>0.11</version>
            <scope>test</scope>
        </dependency>
        <!-- scala test end -->

        <!-- spark start -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_${scala.binary.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-launcher_${scala.binary.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-network-shuffle_${scala.binary.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_${scala.binary.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_${scala.binary.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-catalyst_${scala.binary.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-catalyst_${scala.binary.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming_${scala.binary.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-graphx_${scala.binary.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-mllib_${scala.binary.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming-flume_${scala.binary.version}</artifactId>
            <version>2.4.8</version>
            <exclusions>
                <exclusion>
                    <groupId>log4j</groupId>
                    <artifactId>log4j</artifactId>
                </exclusion>
                <exclusion>
                    <groupId>org.slf4j</groupId>
                    <artifactId>slf4j-log4j12</artifactId>
                </exclusion>
            </exclusions>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming-kafka_2.11</artifactId>
            <version>1.6.3</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming-kafka-0-10_${scala.binary.version}</artifactId>
            <version>${spark.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql-kafka-0-10_${scala.binary.version}</artifactId>
            <version>${spark.version}</version>
            <scope>provided</scope>
        </dependency>


        <dependency>
            <groupId>org.mongodb.spark</groupId>
            <artifactId>mongo-spark-connector_${scala.binary.version}</artifactId>
            <version>3.0.0</version>
        </dependency>
        <dependency>
            <groupId>com.datastax.spark</groupId>
            <artifactId>spark-cassandra-connector_${scala.binary.version}</artifactId>
            <version>3.0.0-beta</version>
        </dependency>

        <dependency>
            <groupId>org.elasticsearch</groupId>
            <artifactId>elasticsearch-spark-20_2.11</artifactId>
            <version>7.4.2</version>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-avro_${scala.binary.version}</artifactId>
            <version>${spark.version}</version>
        </dependency>

        <!-- spark end -->

        <!-- log start -->
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-api</artifactId>
            <version>${slf4j.version}</version>
        </dependency>
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-log4j12</artifactId>
            <version>${slf4j.version}</version>
        </dependency>
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>jul-to-slf4j</artifactId>
            <version>${slf4j.version}</version>
        </dependency>
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>jcl-over-slf4j</artifactId>
            <version>${slf4j.version}</version>
            <!-- runtime scope is appropriate, but causes SBT build problems -->
        </dependency>
        <dependency>
            <groupId>log4j</groupId>
            <artifactId>log4j</artifactId>
            <version>${log4j.version}</version>
        </dependency>
        <!-- log end -->

        <!-- tools start -->
        <dependency>
            <groupId>com.google.code.gson</groupId>
            <artifactId>gson</artifactId>
            <version>${gson.version}</version>
        </dependency>
        <dependency>
            <groupId>org.projectlombok</groupId>
            <artifactId>lombok</artifactId>
            <version>${lombok.version}</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>com.google.guava</groupId>
            <artifactId>guava</artifactId>
            <version>${guava.version}</version>
        </dependency>
        <!-- tools end -->
    </dependencies>

    <build>
        <pluginManagement>
            <plugins>
                <plugin>
                    <groupId>net.alchim31.maven</groupId>
                    <artifactId>scala-maven-plugin</artifactId>
                    <!-- SPARK-36547: Please don't upgrade the version below, otherwise there will be an error on building Hadoop 2.7 package -->
                    <version>4.3.0</version>
                    <executions>
                        <execution>
                            <id>eclipse-add-source</id>
                            <goals>
                                <goal>add-source</goal>
                            </goals>
                        </execution>
                        <execution>
                            <id>scala-compile-first</id>
                            <goals>
                                <goal>compile</goal>
                            </goals>
                        </execution>
                        <execution>
                            <id>scala-test-compile-first</id>
                            <goals>
                                <goal>testCompile</goal>
                            </goals>
                        </execution>
                        <execution>
                            <id>attach-scaladocs</id>
                            <phase>verify</phase>
                            <goals>
                                <goal>doc-jar</goal>
                            </goals>
                        </execution>
                    </executions>
                    <configuration>
                        <scalaVersion>${scala.version}</scalaVersion>
                        <checkMultipleScalaVersions>true</checkMultipleScalaVersions>
                        <failOnMultipleScalaVersions>true</failOnMultipleScalaVersions>
                        <recompileMode>incremental</recompileMode>
                        <args>
                            <arg>-unchecked</arg>
                            <arg>-deprecation</arg>
                            <arg>-feature</arg>
                            <arg>-explaintypes</arg>
                            <arg>-target:jvm-1.8</arg>
                            <arg>-Xfatal-warnings</arg>
                            <arg>-Ywarn-unused:imports</arg>
                            <arg>-P:silencer:globalFilters=.*deprecated.*</arg>
                        </args>
                        <jvmArgs>
                            <jvmArg>-Xss128m</jvmArg>
                            <jvmArg>-Xms4g</jvmArg>
                            <jvmArg>-Xmx6g</jvmArg>
                            <jvmArg>-XX:MaxMetaspaceSize=2g</jvmArg>
                            <jvmArg>-XX:ReservedCodeCacheSize=4g</jvmArg>
                        </jvmArgs>
                        <javacArgs>
                            <javacArg>-source</javacArg>
                            <javacArg>${java.version}</javacArg>
                            <javacArg>-target</javacArg>
                            <javacArg>${java.version}</javacArg>
                            <javacArg>-Xlint:all,-serial,-path,-try</javacArg>
                        </javacArgs>
                        <compilerPlugins>
                            <compilerPlugin>
                                <groupId>com.github.ghik</groupId>
                                <artifactId>silencer-plugin_${scala.version}</artifactId>
                                <version>1.7.6</version>
                            </compilerPlugin>
                        </compilerPlugins>
                    </configuration>
                </plugin>
                <plugin>
                    <groupId>org.apache.maven.plugins</groupId>
                    <artifactId>maven-compiler-plugin</artifactId>
                    <version>3.8.1</version>
                    <configuration>
                        <source>${java.version}</source>
                        <target>${java.version}</target>
                        <skipMain>true</skipMain> <!-- skip compile -->
                        <skip>true</skip> <!-- skip testCompile -->
                    </configuration>
                </plugin>
                <!-- Surefire runs all Java tests -->
                <plugin>
                    <groupId>org.apache.maven.plugins</groupId>
                    <artifactId>maven-surefire-plugin</artifactId>
                    <version>3.0.0-M5</version>
                    <!-- Note config is repeated in scalatest config -->
                    <configuration>
                        <includes>
                            <include>**/Test*.java</include>
                            <include>**/*Test.java</include>
                            <include>**/*TestCase.java</include>
                            <include>**/*Suite.java</include>
                        </includes>
                        <reportsDirectory>${project.build.directory}/surefire-reports</reportsDirectory>
                        <argLine>-ea -Xmx4g -Xss4m -XX:MaxMetaspaceSize=2g -XX:ReservedCodeCacheSize=${CodeCacheSize}
                            ${extraJavaTestArgs} -Dio.netty.tryReflectionSetAccessible=true
                        </argLine>
                        <environmentVariables>
                            <!--
                              Setting SPARK_DIST_CLASSPATH is a simple way to make sure any child processes
                              launched by the tests have access to the correct test-time classpath.
                            -->
                            <SPARK_DIST_CLASSPATH>${test_classpath}</SPARK_DIST_CLASSPATH>
                            <SPARK_PREPEND_CLASSES>1</SPARK_PREPEND_CLASSES>
                            <SPARK_SCALA_VERSION>${scala.binary.version}</SPARK_SCALA_VERSION>
                            <SPARK_TESTING>1</SPARK_TESTING>
                            <JAVA_HOME>${test.java.home}</JAVA_HOME>
                        </environmentVariables>
                        <systemProperties>
                            <derby.system.durability>test</derby.system.durability>
                            <java.awt.headless>true</java.awt.headless>
                            <java.io.tmpdir>${project.build.directory}/tmp</java.io.tmpdir>
                            <spark.test.home>${spark.test.home}</spark.test.home>
                            <spark.testing>1</spark.testing>
                            <spark.master.rest.enabled>false</spark.master.rest.enabled>
                            <spark.ui.enabled>false</spark.ui.enabled>
                            <spark.ui.showConsoleProgress>false</spark.ui.showConsoleProgress>
                            <spark.unsafe.exceptionOnMemoryLeak>true</spark.unsafe.exceptionOnMemoryLeak>
                            <spark.memory.debugFill>true</spark.memory.debugFill>
                            <spark.hadoop.hadoop.security.key.provider.path>test:///
                            </spark.hadoop.hadoop.security.key.provider.path>
                            <!-- Needed by sql/hive tests. -->
                            <test.src.tables>src</test.src.tables>
                        </systemProperties>
                        <failIfNoTests>false</failIfNoTests>
                        <excludedGroups>${test.exclude.tags}</excludedGroups>
                        <groups>${test.include.tags}</groups>
                    </configuration>
                    <executions>
                        <execution>
                            <id>test</id>
                            <goals>
                                <goal>test</goal>
                            </goals>
                        </execution>
                    </executions>
                </plugin>
                <!-- Scalatest runs all Scala tests -->
                <plugin>
                    <groupId>org.scalatest</groupId>
                    <artifactId>scalatest-maven-plugin</artifactId>
                    <version>${scalatest-maven-plugin.version}</version>
                    <!-- Note config is repeated in surefire config -->
                    <configuration>
                        <reportsDirectory>${project.build.directory}/surefire-reports</reportsDirectory>
                        <junitxml>.</junitxml>
                        <filereports>SparkTestSuite.txt</filereports>
                        <argLine>-ea -Xmx4g -Xss4m -XX:MaxMetaspaceSize=2g -XX:ReservedCodeCacheSize=${CodeCacheSize}
                            ${extraJavaTestArgs} -Dio.netty.tryReflectionSetAccessible=true
                        </argLine>
                        <stderr/>
                        <environmentVariables>
                            <!--
                              Setting SPARK_DIST_CLASSPATH is a simple way to make sure any child processes
                              launched by the tests have access to the correct test-time classpath.
                            -->
                            <SPARK_DIST_CLASSPATH>${test_classpath}</SPARK_DIST_CLASSPATH>
                            <SPARK_PREPEND_CLASSES>1</SPARK_PREPEND_CLASSES>
                            <SPARK_SCALA_VERSION>${scala.binary.version}</SPARK_SCALA_VERSION>
                            <SPARK_TESTING>1</SPARK_TESTING>
                            <JAVA_HOME>${test.java.home}</JAVA_HOME>
                        </environmentVariables>
                        <systemProperties>
                            <derby.system.durability>test</derby.system.durability>
                            <java.awt.headless>true</java.awt.headless>
                            <java.io.tmpdir>${project.build.directory}/tmp</java.io.tmpdir>
                            <spark.test.home>${spark.test.home}</spark.test.home>
                            <spark.testing>1</spark.testing>
                            <spark.ui.enabled>false</spark.ui.enabled>
                            <spark.ui.showConsoleProgress>false</spark.ui.showConsoleProgress>
                            <spark.unsafe.exceptionOnMemoryLeak>true</spark.unsafe.exceptionOnMemoryLeak>
                            <spark.test.webdriver.chrome.driver>${spark.test.webdriver.chrome.driver}
                            </spark.test.webdriver.chrome.driver>
                            <spark.test.docker.keepContainer>${spark.test.docker.keepContainer}
                            </spark.test.docker.keepContainer>
                            <spark.test.docker.removePulledImage>${spark.test.docker.removePulledImage}
                            </spark.test.docker.removePulledImage>
                            <!-- Needed by sql/hive tests. -->
                            <test.src.tables>__not_used__</test.src.tables>
                        </systemProperties>
                        <tagsToExclude>${test.exclude.tags},${test.default.exclude.tags}</tagsToExclude>
                        <tagsToInclude>${test.include.tags}</tagsToInclude>
                    </configuration>
                    <executions>
                        <execution>
                            <id>test</id>
                            <goals>
                                <goal>test</goal>
                            </goals>
                        </execution>
                    </executions>
                </plugin>
                <!-- Enable surefire and scalatest in all children, in one place: -->
                <plugin>
                    <groupId>org.apache.maven.plugins</groupId>
                    <artifactId>maven-surefire-plugin</artifactId>
                </plugin>
                <plugin>
                    <groupId>org.scalatest</groupId>
                    <artifactId>scalatest-maven-plugin</artifactId>
                </plugin>
            </plugins>
        </pluginManagement>
    </build>
</project>
